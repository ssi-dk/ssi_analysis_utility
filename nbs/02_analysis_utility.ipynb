{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp analysis_utility\n",
    "# You need this at the top of every notebook you want turned into a module, the name your provide will determine the module name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# That export there, it makes sure this code goes into the module.\n",
    "\n",
    "# standard libs\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Common to template\n",
    "# add into settings.ini, requirements, package name is python-dotenv, for conda build ensure `conda config --add channels conda-forge`\n",
    "import dotenv  # for loading config from .env files, https://pypi.org/project/python-dotenv/\n",
    "import envyaml  # Allows to loads env vars into a yaml file, https://github.com/thesimj/envyaml\n",
    "import fastcore  # To add functionality related to nbdev development, https://github.com/fastai/fastcore/\n",
    "from fastcore import (\n",
    "    test,\n",
    ")\n",
    "from fastcore.script import (\n",
    "    call_parse,\n",
    ")  # for @call_parse, https://fastcore.fast.ai/script\n",
    "import json  # for nicely printing json and yaml\n",
    "from fastcore import test\n",
    "from fastcore.script import call_parse\n",
    "from ssi_analysis_utility import (\n",
    "    core,\n",
    "    sample_manager,\n",
    "    convert_external_genome,\n",
    "    \n",
    ")\n",
    "from pathlib import Path #to be able write :Path in cli function\n",
    "\n",
    "# Project specific libraries\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change working directory when running notebook only. DO NOT EXPORT\n",
    "\n",
    "os.chdir(core.PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis utility\n",
    "\n",
    "The generic analysis_utility class inherits variables from the sample_data class: sample_name, file paths (assembly, Illumina, Nanopore), metadata etc.\n",
    "One extra attribute is added in the initiation of analysis_utility: an output folder to store the analysis results (\"analysis_utility.analysis_folder)\n",
    "\n",
    "It contains methods for generic analysis and functionality that are used across species such as:\n",
    "\n",
    "- Blasting genes and parsing output for presence absence of those genes\n",
    "- Generating vcf file from Illumina reads (running bwa-mem and samtools against provided reference, can be used for both genes and whole genome sequences. The vcf output is formatted to be compatible with NASP)\n",
    "- Parsing vcf file to identify variants at specified positions (for lineage determination and identification of specific SNPs in genes)\n",
    "- Generating frankenfasta file from assembly (Aligning assembly to provided reference using NUCMER. Frankenfasta output can be concatenated with frankenfastas from other isolates to obtain a whole genome alignment if they use the same reference)\n",
    "- Parsing frankenfasta file to identify variants at specified positions (for lineage determination and identification of specific SNPs in genes)\n",
    "\n",
    "\n",
    "## Standards\n",
    "\n",
    "Each analysis has a defined alias, f.ex. \"virulence_gene_detection\", \"emm_typing\" or \"lineage_determination\". This alias is provided as a parameter when calling the method that does the analysis, and results and output files for each analysis are stored in dicts as parameters .analysis_results and .analysis_output_files\n",
    "\n",
    ".analysis_results should be a dictionary of dictionaries with strings as values. The top level key should be the analysis alias, and the keys in the second tier will be the tsv-headers in the final summary output, while the values will be printed to the field under that header.\n",
    "\n",
    "Like this:\n",
    "\n",
    "    analysis_alias_1:\n",
    "        Test1: Result 1\n",
    "        Test2: Result 2\n",
    "    analysis_alias_2:\n",
    "        Test1: Result 1\n",
    "    analysis_alias_3:\n",
    "        Test1: Result 1\n",
    "        ....\n",
    "\n",
    "\n",
    "For example if the .analysis_result attribute looks like this after all analyses are run\n",
    "\n",
    "    \"emm_typing\":\n",
    "        \"emm_type\": \"4.0\"\n",
    "        \"enn_type\": \"203.3\"\n",
    "        ...\n",
    "    \"lineage_determination\":\n",
    "        \"Lineage\": \"M4 cluster 2\"\n",
    "    \"virulence_gene_detection:\n",
    "        \"speA\": 0\n",
    "        \"speB\": 1\n",
    "        \"speC\": 1\n",
    "\n",
    "\n",
    "The summary output tsv will look like this:\n",
    "\n",
    "\n",
    "sample_name | assembly_file   |    ....... | emm_type | enn_type  | ... | Lineage     |  speA | speB | speC\n",
    ":---- | :----- | :----- | :----- | :----- | :----- | :----- | :----- | :----- | :-----\n",
    "GAS-2024-0773 | GAS-2024-0773.fasta | .......|  4.0    |   203.3  | ... | M4 cluster 2 | 0   |  1  |   1\n",
    "\n",
    "\n",
    "By default, files generated for analysis are printed to /provided/output/path/[analysis_alias]\n",
    "\n",
    "# An analysis may use multiple methods\n",
    "F.ex. lineage_determination consists of three steps;\n",
    "1. mapping and variant calling based on reads or assembly data\n",
    "    - Using methods ._Illumina_read_mapping or ._nucmer_mapping\n",
    "2. parsing the vcf or frankenfasta output for variant positions matching those provided in a specific file\n",
    "    - Using methods .get_SNPs_from_vcf or .get_SNPs_from_fasta\n",
    "3. converting that information to a lineage.\n",
    "\n",
    ".\\_assembly_lineage_determination\\_ or ._Illumina_lineage_determination\\_ performs all three of the above and therefore the whole analysis in one method\n",
    "\n",
    "Methods that start with \"\\_\" use input contained in the attributes as input, i.e. assembly file or read files. It may also require other input, like a reference file to map against, but it can be used as a first step in an anlysis. If a method does not beging with \"\\_\", that means it needs the output from a different method as input.\n",
    "\n",
    "Methods that end in \"_\" are final steps of an analysis, i.e. these will update the dictionary in analysis_utility.analysis_results[analysis_alias] with results from the \n",
    "it's output needs to be passed on to another method to get results. These do not add anything to the .analysis_results dictionary, but may add to the .analysis_output_files dictionary. It should also return what is relevant for downstream analysis (f.ex. a path to a vcf-file, or one or more dictinaries and/or dataframes).\n",
    "\n",
    "## Other notes:\n",
    "\n",
    "### Naming requirements\n",
    "Do _not_ use commas (,), colons (:) and spaces in your analysis_alias names\n",
    "\n",
    "\n",
    "### Keeping intermediate files\n",
    "\n",
    "The config for every analysis should have a \"files_to_clean\" key with a list of files that are deleted after analysis is run. Simply edit that list in the config before calling the function to only delete a subset of files\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "class analysis_utility(sample_manager.sample_data):\n",
    "    \"\"\"\n",
    "    Analysis_utility class, which extends sample_manager.sample_data\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the log file name for logging analysis processes\n",
    "    log_file_name = \"analysis.log\"\n",
    "\n",
    "    # Initialize the analysis_utility class with provided attributes and settings\n",
    "    def __init__(self,\n",
    "                 attributes,\n",
    "                 input_folder,\n",
    "                 output_folder,\n",
    "                 analysis_config):\n",
    "        \"\"\"\n",
    "        Initialize the analysis utility class by setting up attributes and configurations.\n",
    "\n",
    "        Parameters:\n",
    "        -------------\n",
    "        attributes : dict\n",
    "            The attributes related to the sample data.\n",
    "        input_folder : str\n",
    "            Path to the folder where input files are located.\n",
    "        output_folder : str\n",
    "            Path where the analysis results will be saved.\n",
    "        analysis_config : dict\n",
    "            Configuration settings for the analysis, including analysis type and parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        attributes = attributes.copy() # Make a copy of attributes to avoid modifying original data\n",
    "        super().__init__(attributes,\n",
    "                         input_folder) # Call the parent class initializer\n",
    "        self.analysis_results = {} #  Initialize an empty dictionary for storing analysis results\n",
    "        self.analysis_output_files = {} ## Initialize an empty dictionary for storing analysis output files\n",
    "        self.output_folder = output_folder ## Set the output folder for saving results\n",
    "        self.analysis_config = analysis_config # Store the analysis configuration\n",
    "        self.check_path_existence() # Check the existence of paths provided in the configuration\n",
    "\n",
    "    # Setup up log filehandle for sample\n",
    "    def setup_sample_logging(self):\n",
    "        \"\"\"\n",
    "        Method to initialize logging file for the sample to track the analysis process.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = logging.getLogger() # Get the logger instance\n",
    "        log_file = os.path.join(self.output_folder,\n",
    "                                self.log_file_name) # Define the log file path\n",
    "        self.log_filehandle = logging.FileHandler(log_file, \n",
    "                                                  mode = \"w\",\n",
    "                                                  encoding = \"utf-8\") # Create the log file handler\n",
    "        self.log_filehandle.setFormatter(logging.Formatter(\n",
    "            fmt=\"{asctime} - {levelname} - {message}\",\n",
    "            style=\"{\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "            )) # Log and date format \n",
    "        self.logger.addHandler(self.log_filehandle)\n",
    "        self.logger.setLevel(logging.INFO) # Set the logging level to INFO\n",
    "\n",
    "    def check_path_existence(self):\n",
    "        \"\"\"\n",
    "        Method to verify the existence of the provided input files (assembly, Illumina, Nanopore).\n",
    "        If a file does not exist, it is set to None and a warning is logged.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the assembly file exists\n",
    "        if not self.assembly_file is None and not os.path.exists(self.assembly_file):\n",
    "            self.logger.warning(f\"Provided assembly file does not exist at {self.assembly_file}. Analyses dependent on assembly input will be skipped\")\n",
    "            self.assembly_file = None\n",
    "        # Check if the Illumina read files exist (R1 and R2)\n",
    "        if not self.Illumina_read_files is None and not os.path.exists(self.Illumina_read_files[0]):\n",
    "            self.logger.warning(f\"Provided paired end R1 file does not exist at {self.Illumina_read_files[0]}. Analyses dependent on paired end input will be skipped\")\n",
    "            self.Illumina_read_files = None\n",
    "        if not self.Illumina_read_files is None and not os.path.exists(self.Illumina_read_files[1]):\n",
    "            self.logger.warning(f\"Provided paired end R2 file does not exist at {self.Illumina_read_files[1]}. Analyses dependent on paired end input will be skipped\")\n",
    "            self.Illumina_read_files = None\n",
    "        # Check if the Nanopore read file exists\n",
    "        if not self.Nanopore_read_file is None and not os.path.exists(self.Nanopore_read_file):\n",
    "            self.logger.warning(f\"Provided Nanopore data file does not exist at {self.Nanopore_read_file}. Analyses dependent on Nanopore input will be skipped\")\n",
    "            self.Nanopore_read_file = None\n",
    "\n",
    "\n",
    "    # Set up folder to store results for specific analysis. \n",
    "    def analysis_setup(self,\n",
    "                       analysis_alias,\n",
    "                       output_folder= False):\n",
    "        \"\"\"\n",
    "        Sets up a folder for saving the results of a specific analysis.\n",
    "        Default path: [output_folder]/[sample_name]/[analysis_alias]\n",
    "\n",
    "        Parameters:\n",
    "        -------------\n",
    "        analysis_alias : str\n",
    "            The alias/name of the analysis being run.\n",
    "        output_folder : str, optional\n",
    "            The folder path for saving the results. If False, a default path will be used.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str : The output folder path where results will be saved.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Log the analysis being run\n",
    "        self.logger.info(f\"------------------------------------------------------------------------------------------\")\n",
    "        self.logger.info(f\"RUNNING ANALYSIS: {analysis_alias}\")\n",
    "        if not output_folder:\n",
    "            output_folder = os.path.join(self.output_folder,analysis_alias) # Set the default output folder\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder) # Create the output folder if it doesn't exist\n",
    "        if analysis_alias in self.analysis_results:\n",
    "            self.logger.warning(f\"An analysis with name {analysis_alias} has already been performed. Outputs in {output_folder} may be overwritten.\")\n",
    "        self.logger.info(f\"Printing {analysis_alias} results for {self.sample_name} to {output_folder}\")  # Log the output location\n",
    "        self.analysis_results[analysis_alias] = {} ## Initialize an empty dictionary for the analysis results\n",
    "        self.analysis_output_files[analysis_alias] = {} ## Initialize an empty dictionary for output files\n",
    "        return(output_folder)\n",
    "\n",
    "    \n",
    "    def sample_setup(self):\n",
    "        \"\"\"\n",
    "        Method to initialize the folder for the sample and sets up logging for the analysis.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.output_folder):\n",
    "            try:\n",
    "                os.makedirs(self.output_folder) # Create the output folder for the sample if it doesn't exist\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to create folder for {self.sample_name} in {self.output_folder}. {e}\")\n",
    "                #self.logger.critical(f\"Failed to create folder for {self.sample_name} in {self.output_folder}. {e}\")\n",
    "        self.setup_sample_logging() # Set up the logging for the sample\n",
    "        self.logger.info(f\"Running analyses on {self.sample_name} in {self.output_folder}\")\n",
    "        self.logger.info(f\"Inputs:\")  # Log input data\n",
    "        if not self.assembly_file is None:\n",
    "            self.logger.info(f\"Assembly file: {self.assembly_file}\")\n",
    "        if not self.Illumina_read_files is None:\n",
    "            self.logger.info(f\"Illumina read files: {', '.join(self.Illumina_read_files)}\")\n",
    "        if not self.Nanopore_read_file is None:\n",
    "            self.logger.info(f\"Nanopore read file: {self.Nanopore_read_file}\")\n",
    "        self.logger.info(f\"------------------------------------------------------------------------------------------\")\n",
    "        self.logger.info(f\"Analyses to run: {', '.join(self.analysis_config['analyses_to_run'])}\")\n",
    "\n",
    " \n",
    "    def sample_cleanup(self):\n",
    "        \"\"\"\n",
    "        Method to clean up logging resources by removing the log file handler.\n",
    "        \"\"\"\n",
    "        self.logger.removeHandler(self.log_filehandle) # Remove the file handler\n",
    "        self.log_filehandle.close() # Close the log file\n",
    "\n",
    " \n",
    "\n",
    "    def analysis_cleanup(self,\n",
    "                         analysis_alias,\n",
    "                         files_to_clean):\n",
    "        \"\"\"\n",
    "        Cleans up intermediate output files generated during an analysis.\n",
    "        To be run as the last step of any analysis after updating self.analysis_results\n",
    "\n",
    "        Parameters:\n",
    "        -------------\n",
    "        analysis_alias : str\n",
    "            The alias of the analysis whose temporary files need to be cleaned.\n",
    "        files_to_clean : list\n",
    "            A list of file names to be cleaned up after analysis.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Cleaning temporary files from analysis {analysis_alias}\")\n",
    "        output_files = self.analysis_output_files[analysis_alias] # Get the output files dictionary for the analysis\n",
    "        output_file_folders = set() # Set to keep track of output file directories\n",
    "        \n",
    "        # Iterate over files to clean and delete them\n",
    "        for file_to_clean in files_to_clean:\n",
    "            if os.path.exists(output_files[file_to_clean]):\n",
    "                self.logger.info(f\"Removing {output_files[file_to_clean]}\") # Log the file removal\n",
    "                os.remove(output_files[file_to_clean]) # Remove the file\n",
    "            output_file_folders.add(os.path.dirname(output_files[file_to_clean]))  # Add the directory to the set\n",
    "        \n",
    "        # Remove empty directories after cleaning files\n",
    "        for path in output_file_folders:  \n",
    "            if os.path.exists(path) and not os.path.isfile(path) and not os.listdir(path):\n",
    "                try:    \n",
    "                    self.logger.info(f\"Cleaned all files from {path}. Removing empty directory\")\n",
    "                    os.rmdir(path) # Remove the empty directory\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Attempt to remove directory {path} failed. {e}\")\n",
    "\n",
    " \n",
    "    def write_to_tsv(self,\n",
    "                     include_metadata = True):\n",
    "        \"\"\"\n",
    "        Write results from all analyses to .tsv file. Includes metadata by default.\n",
    "\n",
    "        Parameters:\n",
    "        -------------\n",
    "\n",
    "        include_metadata : bool\n",
    "            Wether to include metadata\n",
    "        \"\"\"\n",
    "        \n",
    "        output_file = os.path.join(self.output_folder,\n",
    "                                   \"results_summary.tsv\") # Where to store the file\n",
    "        if include_metadata:\n",
    "            print_dict = self.metadata.copy() # Include metadata\n",
    "            # If 'Illumina_read_files' is a list, convert it to a comma-separated string\n",
    "            if 'Illumina_read_files' in print_dict and isinstance(print_dict['Illumina_read_files'], list):\n",
    "                print_dict[\"Illumina_read_files\"] = \",\".join(print_dict[\"Illumina_read_files\"])\n",
    "             # Move 'sample_name' to the first column for readability\n",
    "            if \"sample_name\" in print_dict:\n",
    "                print_dict = {'sample_name': print_dict.pop('sample_name'), **print_dict} \n",
    "        else:\n",
    "            # Only include 'sample_name' in the output if metadata is excluded\n",
    "            if \"sample_name\" in print_dict:\n",
    "                print_dict = {\"sample_name\": self.sample_name}\n",
    "            else:\n",
    "                print_dict = {}\n",
    "        \n",
    "        # Add analysis results to the dictionary to be printed\n",
    "        for analysis_alias in self.analysis_results:\n",
    "            print_dict.update(self.analysis_results[analysis_alias])\n",
    "        \n",
    "        # Write the results to the output file, catching any exceptions\n",
    "        try:\n",
    "            with open(output_file, 'w') as o:\n",
    "                dict_writer = csv.DictWriter(o, fieldnames=list(print_dict.keys()), delimiter='\\t')\n",
    "                dict_writer.writeheader()\n",
    "                dict_writer.writerows([print_dict])  \n",
    "            o.close()\n",
    "            self.logger.info(f\"Analysis summary written to {output_file}\")\n",
    "        except Exception as e:\n",
    "            self.logger.critical(f\"ERROR while writing analysis summary to {output_file}: {e}\")\n",
    "\n",
    "\n",
    "   \n",
    "    def execute_cmd_and_log(self,\n",
    "                            cmd,\n",
    "                            log_stdout = True):\n",
    "        \"\"\"\n",
    "        Method to execute a shell command, logging stdout and stderr automatically.\n",
    "        \n",
    "        Parameters:\n",
    "        -------------\n",
    "        cmd : str\n",
    "            The command to be executed.\n",
    "        log_stdout : bool\n",
    "            Whether to log stdout.\n",
    "        \n",
    "        Returns:\n",
    "        -------------\n",
    "        stdout : str\n",
    "            Standard output from the command.\n",
    "        stderr : str\n",
    "            Standard error from the command.\n",
    "    \"\"\"     \n",
    "        \n",
    "        # Run the shell command and capture output\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True, encoding='utf-8')\n",
    "        stdout,stderr = process.communicate()\n",
    "        \n",
    "        # Log the command and its output\n",
    "        self.logger.info(f\"Running command: {cmd}\")\n",
    "        if log_stdout and stdout and stdout is not None:\n",
    "            self.logger.info(f\"Shell command STDOUT: {stdout}\")\n",
    "        if stderr and stderr is not None:\n",
    "            self.logger.error(f\"Shell command STDERR: {stderr}\")\n",
    "        return(stdout,stderr)\n",
    "\n",
    "\n",
    "    ### Analysis functions\n",
    "    \n",
    "    ## Below are functions involved in running individual analyses\n",
    "\n",
    "    # \n",
    "    # \n",
    "    # VCF output can be used directly in nasp\n",
    "    def _Illumina_read_mapping(self,\n",
    "                               analysis_alias,\n",
    "                               reference_fasta_file, \n",
    "                               output_folder = False):\n",
    "        \"\"\"\n",
    "        Method to map paired-end Illumina reads to a reference genome \n",
    "        using BWA MEM, then perform variant calling with GATK.\n",
    "        VCF output can be used for lineage determination when mapping \n",
    "        to complete genome and for checking for SNPs in specific genes \n",
    "        (._get_SNPs_from_vcf_)\n",
    "        Map paired end Illumina reads to provided reference with bwa mem \n",
    "        and do variant calling with gatk. Returns path to vcf-file.\n",
    "        \n",
    "        Parameters:\n",
    "        -------------\n",
    "        analysis_alias : str\n",
    "            Alias for this analysis.\n",
    "        reference_fasta_file : str\n",
    "            Path to the reference FASTA file.\n",
    "        output_folder : str\n",
    "            Folder to store output files. Default is the main output folder.\n",
    "        \n",
    "        Returns:\n",
    "        -------------\n",
    "        vcf_file : str\n",
    "            Path to the generated VCF file.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set output folder and file paths\n",
    "        if not output_folder:\n",
    "            output_folder = os.path.join(self.output_folder,analysis_alias)\n",
    "        bam_prefix = os.path.join(output_folder,self.sample_name+\"-bwamem\")\n",
    "        bam_file = bam_prefix+\".bam\"\n",
    "        vcf_file = os.path.join(output_folder,self.sample_name+\".vcf\")\n",
    "        \n",
    "        # Define commands for BWA, Samtools, and GATK\n",
    "        bwamem_cmd = f\"bwa mem -R \\'@RG\\\\tID:{self.sample_name}\\\\tSM:{self.sample_name}\\'  -t 4 {reference_fasta_file} {self.Illumina_read_files[0]} {self.Illumina_read_files[1]} | samtools view -S -b -h - | samtools sort -o {bam_file}\"\n",
    "        samtoolsindex_cmd =  f\"samtools index {bam_file}\"\n",
    "        gatk_cmd = f\"java -Xmx10G -jar GenomeAnalysisTK.jar -T UnifiedGenotyper -dt NONE -glm BOTH -I {bam_file} -R {reference_fasta_file} -nt 4 -o {vcf_file} -out_mode EMIT_ALL_CONFIDENT_SITES -baq RECALCULATE -stand_call_conf 100 -ploidy 1\"\n",
    "        commands = {}\n",
    "        \n",
    "        # Execute the appropriate commands based on the existing files\n",
    "        if os.path.exists(vcf_file):\n",
    "            self.logger.info(f\"vcf file found at {vcf_file}. Skipping read mapping\")\n",
    "        else:\n",
    "            if os.path.exists(bam_file+\".bai\"):\n",
    "                cmd = gatk_cmd\n",
    "                self.logger.info(f\"Indexed bam file found at {bam_file}.bai. Running gatk.\")\n",
    "            else:\n",
    "                if os.path.exists(bam_file):\n",
    "                    cmd = f\"{samtoolsindex_cmd}; {gatk_cmd}\"\n",
    "                    self.logger.info(f\"Bam file found at {bam_file}. Indexing and running gatk.\")\n",
    "                else:\n",
    "                    cmd = f\"{bwamem_cmd}; {samtoolsindex_cmd}; {gatk_cmd}\"\n",
    "                    self.logger.info(f\"Running bwamem and gatk\")\n",
    "            \n",
    "            # Run the constructed command\n",
    "            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True, encoding='utf-8')\n",
    "            commands[cmd] = process.communicate()\n",
    "            \n",
    "            # Save output files to dictionary\n",
    "            self.analysis_output_files[analysis_alias] = {\"vcf\": vcf_file, \"bam\": bam_file, \"bam_index\": bam_file+\".bai\"}\n",
    "        return(vcf_file)\n",
    "\n",
    "\n",
    "    def get_SNPs_from_vcf(self,\n",
    "                          analysis_alias,\n",
    "                          genotype_variant_table,\n",
    "                          vcf_file):\n",
    "        \"\"\"\n",
    "        Parse a VCF file and a genotype variant table (from ._Illumina_read_mapping) to identify  \n",
    "        variants matching those listed in the genotype_variant_table \n",
    "\n",
    "        Parameters:\n",
    "        -------------\n",
    "        analysis_alias : str\n",
    "            Alias for the analysis.\n",
    "        genotype_variant_table : str\n",
    "            Path to the genotype variant table file.\n",
    "        vcf_file : str\n",
    "            Path to the VCF file.\n",
    "        \n",
    "        Returns:\n",
    "        -------------\n",
    "        LOC_df : pd.DataFrame\n",
    "            DataFrame containing all variants listed in the genotype variant table.\n",
    "        vcf_df : pd.DataFrame\n",
    "            DataFrame containing the complete VCF file.\n",
    "        LOC_df_filtered : pd.DataFrame\n",
    "            DataFrame containing variants found in both the variant table and VCF file.\n",
    "        \"\"\"       \n",
    "       \n",
    "        # Load the VCF and variant table files into dataframes\n",
    "        vcf_df = pd.read_csv(vcf_file, sep='\\t', header = None,comment=\"#\")\n",
    "        vcf_df.columns = [\"referenceID\",\"position\",\"ID\",\"ref\",\"alt\",\"qual\",\"filter\",\"info\",\"format\",\"count\"]\n",
    "        LOC_df = pd.read_csv(genotype_variant_table, sep='\\t')\n",
    "         \n",
    "        # Create a unique variant identifier for matching between the two files\n",
    "        LOC_df[\"variant\"] = LOC_df[\"referenceID\"] + \"::\" + LOC_df[\"ref\"] + LOC_df[\"position\"].astype(str) + LOC_df[\"alt\"]\n",
    "        vcf_df[\"variant\"] = vcf_df[\"referenceID\"] + \"::\" + vcf_df[\"ref\"] + vcf_df[\"position\"].astype(str) + vcf_df[\"alt\"]\n",
    "        \n",
    "         # Filter the VCF DataFrame based on the variants in the variant table\n",
    "        variants_filter_vals = LOC_df[\"variant\"]\n",
    "        vcf_df_filtered = vcf_df.query('variant in @variants_filter_vals')\n",
    "        \n",
    "        # Create a DataFrame for variants found in both files\n",
    "        variants_found = list(vcf_df_filtered[\"variant\"])\n",
    "        LOC_df_filtered = LOC_df.query('variant in @variants_found')\n",
    "\n",
    "        # Calculate SNP counts for each genotype\n",
    "        genotype_SNP_counts = {}\n",
    "        for genotype in set(LOC_df[\"genotype\"]):\n",
    "            try:\n",
    "                print(f\"{genotype} {LOC_df_filtered['genotype'].value_counts()[genotype]} {LOC_df['genotype'].value_counts()[genotype]}\")\n",
    "                genotype_SNP_counts[genotype] = {}\n",
    "            except KeyError:\n",
    "                print(f\"{genotype} 0 {LOC_df['genotype'].value_counts()[genotype]}\")\n",
    "        return(LOC_df,vcf_df,LOC_df_filtered)\n",
    "\n",
    "    \n",
    "    def _nucmer_mapping(self,\n",
    "                        analysis_alias,\n",
    "                        reference_fasta_file,\n",
    "                        output_folder = False):\n",
    "        \"\"\"\n",
    "        Maps the assembly sequences to a provided reference using Nucmer, producing a \"frankenfasta\" file.\n",
    "        The frankenfasta output is aligned to the reference and can be used for lineage determination\n",
    "        and SNP detection within specific genes (._get_SNPs_from_frankenfasta).\n",
    "\n",
    "        Parameters:\n",
    "        -------------\n",
    "            analysis_alias : str\n",
    "                    Identifier for the analysis, used to label output files.\n",
    "            reference_fasta_file : str\n",
    "                    Path to the reference fasta file.\n",
    "            output_folder : str \n",
    "                    Directory where output files are saved. Defaults to a subfolder in self.output_folder.\n",
    "\n",
    "        Returns:\n",
    "            str: Path to the generated frankenfasta file.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set the output folder based on provided path or default to a folder named after the analysis alias\n",
    "        if not output_folder:\n",
    "            output_folder = os.path.join(self.output_folder,analysis_alias)\n",
    "        \n",
    "        # Initialize an external genome object and import the assembly fasta file into it\n",
    "        external_genome = convert_external_genome.Genome()\n",
    "        external_genome.import_fasta_file(self.assembly_file)\n",
    "        \n",
    "        # Define file paths based on sample name and output folder\n",
    "        prefix = os.path.join(output_folder,self.sample_name)\n",
    "        delta_file = f\"{prefix}.delta\"\n",
    "        filtered_delta_file = f\"{prefix}.filtered.delta\"\n",
    "        frankenfasta_file = f\"{prefix}.frankenfasta\"\n",
    "        \n",
    "        # Check if frankenfasta file already exists to avoid re-running the process\n",
    "        if not os.path.exists(frankenfasta_file):\n",
    "            self.logger.info(f\"Running nucmer and writing frankenfasta file to {frankenfasta_file}\")\n",
    "            \n",
    "            # Command to run Nucmer with the reference and assembly files\n",
    "            delta_cmd = f\"nucmer --prefix={prefix} {reference_fasta_file} {self.assembly_file}\"\n",
    "            # Execute the delta command and log any errors, then execute the delta-filter command\n",
    "            delta_filter_cmd = f\"delta-filter -q -r -o 100 {delta_file} > {filtered_delta_file}\"\n",
    "            \n",
    "            # Execute the delta command and log any errors, then execute the delta-filter command\n",
    "            stdout,stderr = self.execute_cmd_and_log(delta_cmd,\n",
    "                                                     log_stdout=False)\n",
    "            stdout,stderr = self.execute_cmd_and_log(delta_filter_cmd)\n",
    "            \n",
    "            # Re-initialize genome objects to handle frankenfasta generation\n",
    "            external_genome = convert_external_genome.Genome()\n",
    "            external_genome.import_fasta_file(self.assembly_file)\n",
    "            franken_genome = convert_external_genome.Genome()\n",
    "            \n",
    "            # Parse the filtered delta file to create the frankenfasta genome based on the mapping\n",
    "            convert_external_genome.parse_delta_file(( filtered_delta_file ), \n",
    "                                                     franken_genome, \n",
    "                                                     external_genome)\n",
    "             # Write the frankenfasta genome to a fasta file, adding sample name and reference to the header\n",
    "            franken_genome.write_to_fasta_file(( frankenfasta_file ), self.sample_name + \" ref:\")\n",
    "        else:\n",
    "            self.logger.info(f\"Frankenfasta file already exists at {frankenfasta_file}, skipping nucmer alignment\")\n",
    "        # Store the paths of the generated output files in analysis_output_files for reference\n",
    "        self.analysis_output_files[analysis_alias].update({\"frankenfasta\": frankenfasta_file, \n",
    "                                                           \"delta\": delta_file, \n",
    "                                                           \"filtered_delta\": filtered_delta_file})\n",
    "        return(frankenfasta_file)\n",
    "\n",
    "\n",
    "    def get_SNPs_from_fasta(self,\n",
    "                            variant_table_file,\n",
    "                            mapped_fasta_file):\n",
    "        \"\"\"\n",
    "        Extracts SNPs from a mapped FASTA file by comparing against a variant table.\n",
    "        \n",
    "        Parameters:\n",
    "        -------------\n",
    "\n",
    "        variant_table_file : str \n",
    "                Path to the variant table file in TSV format.\n",
    "        mapped_fasta_file : str\n",
    "                Path to the mapped FASTA file.\n",
    "\n",
    "        Returns:\n",
    "            tuple: DataFrames with all variants and filtered variants based on presence in the FASTA sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        fasta_dict = {} # Dictionary to store sequences from the FASTA file\n",
    "\n",
    "        # Read mapped FASTA file and store sequences in fasta_dict\n",
    "        with open(mapped_fasta_file) as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n')\n",
    "                if line[0] == \">\":\n",
    "                    header = line.split(\"ref:\")[1]\n",
    "                    fasta_dict[header] = \"\"\n",
    "                else:\n",
    "                    fasta_dict[header]+= line\n",
    "        \n",
    "        # Load the variant table and create a variant column with unique SNP identifier\n",
    "        LOC_df = pd.read_csv(variant_table_file, sep='\\t')\n",
    "        LOC_df[\"variant\"] = LOC_df[\"referenceID\"] + \"::\" + LOC_df[\"ref\"] + LOC_df[\"position\"].astype(str) + LOC_df[\"alt\"]\n",
    "\n",
    "        LOC_df_dict = LOC_df.to_dict(\"index\")\n",
    "        variants_filter_vals = LOC_df[\"variant\"]\n",
    "        variants_found = []\n",
    "        \n",
    "        # Check each variant to see if it matches the corresponding sequence in the FASTA file\n",
    "        for idx in LOC_df_dict:\n",
    "            if LOC_df_dict[idx][\"referenceID\"] in fasta_dict:\n",
    "                if fasta_dict[LOC_df_dict[idx][\"referenceID\"]][(LOC_df_dict[idx][\"position\"]-1)] == LOC_df_dict[idx][\"alt\"]:\n",
    "                    variants_found.append(LOC_df_dict[idx][\"variant\"])\n",
    "\n",
    "        LOC_df_filtered = LOC_df.query('variant in @variants_found') # Filter for found variants\n",
    "\n",
    "        return(LOC_df,LOC_df_filtered)\n",
    "\n",
    "    \n",
    "    def _assembly_lineage_determination_(self,\n",
    "                                         lineage_determination_config,\n",
    "                                         mapped_fasta_file = False,\n",
    "                                         output_folder = False):\n",
    "        \"\"\"\n",
    "        Determines the lineage of an assembly by comparing SNP presence against a lineage variant file.\n",
    "\n",
    "        Parameters:\n",
    "        -------------\n",
    "        lineage_determination_config : dict\n",
    "                Configuration for lineage determination.\n",
    "        mapped_fasta_file : str \n",
    "                Path to mapped FASTA file.\n",
    "        output_folder : str\n",
    "                Path to save outputs.\n",
    "\n",
    "        \"\"\"    \n",
    "        \n",
    "        analysis_alias = lineage_determination_config[\"alias\"]\n",
    "        \n",
    "        # Skip analysis without assembly file\n",
    "        if self.assembly_file is None:\n",
    "            self.logger.critical(f\"Assembly file not provided or not found. Skipping analysis {analysis_alias}\")\n",
    "            self.analysis_results[analysis_alias] = {analysis_alias: \"NA\"}\n",
    "        else:\n",
    "            output_folder = self.analysis_setup(analysis_alias,\n",
    "                                                output_folder)\n",
    "            reference_fasta_file = lineage_determination_config[\"reference_fasta_file\"]\n",
    "            variant_table_file = lineage_determination_config[\"lineage_variant_file\"]\n",
    "            \n",
    "            # If no mapped file provided, generate one\n",
    "            if not mapped_fasta_file or not os.path.exists(mapped_fasta_file):\n",
    "                mapped_fasta_file = self._nucmer_mapping(analysis_alias,\n",
    "                                                         reference_fasta_file)\n",
    "            \n",
    "            # Get all and found SNPs\n",
    "            all_variants, found_variants = self.get_SNPs_from_fasta(variant_table_file,\n",
    "                                                                   mapped_fasta_file)\n",
    "            lineage_SNP_counts = {}\n",
    "            \n",
    "            # Calculate lineage SNP percentages\n",
    "            for genotype in set(all_variants[\"genotype\"]):\n",
    "                try:\n",
    "                    lineage_SNP_counts[genotype] = found_variants['genotype'].value_counts()[genotype] / all_variants['genotype'].value_counts()[genotype] * 100\n",
    "                except KeyError:\n",
    "                    lineage_SNP_counts[genotype] = 0\n",
    "            \n",
    "            # Determine the best lineage hit based on percentage threshold\n",
    "            best_hit = max(lineage_SNP_counts, key=lineage_SNP_counts.get)\n",
    "            if not lineage_SNP_counts[best_hit] > lineage_determination_config[\"percent_snp_threshold\"]:\n",
    "                best_hit = \"-\"\n",
    "            \n",
    "            self.analysis_results[analysis_alias].update({\"Lineage\": best_hit})\n",
    "            self.analysis_cleanup(analysis_alias,lineage_determination_config[\"files_to_clean\"])\n",
    "\n",
    "\n",
    "\n",
    "    def _blast_presence_absence_(self,\n",
    "                                 blast_presence_absence_config: dict,\n",
    "                                 output_folder = False):\n",
    "        \"\"\"\n",
    "        Executes BLAST for presence-absence analysis of genes in a query FASTA file.\n",
    "        A gene is considered present if above the threshold specified in config \n",
    "        (default: 90 % identity, 90% length)\n",
    "\n",
    "        Parameters:\n",
    "        -------------\n",
    "        blast_presence_absence_config : dict\n",
    "                Configuration for BLAST parameters.\n",
    "        output_folder : str\n",
    "                Path to save BLAST output.\n",
    "\n",
    "        Returns:\n",
    "            dict: Presence-absence data and genes found.\n",
    "        \"\"\"\n",
    "\n",
    "        analysis_alias = blast_presence_absence_config[\"alias\"]\n",
    "        \n",
    "        # Skip analysis without assembly file\n",
    "        if self.assembly_file is None:\n",
    "            self.logger.critical(f\"Assembly file not provided or not found. Skipping analysis {analysis_alias}\")\n",
    "            self.analysis_results[analysis_alias] = {analysis_alias: \"NA\"}\n",
    "        else:\n",
    "            # Set up options\n",
    "            output_folder = self.analysis_setup(analysis_alias,output_folder)\n",
    "            query_sequence_file = blast_presence_absence_config[\"query_fasta_file\"]\n",
    "            blast_output_file = os.path.join(output_folder,\n",
    "                                             \"blast_output.tsv\")\n",
    "            cov_threshold = blast_presence_absence_config[\"cov_threshold\"]\n",
    "            pident_threshold = blast_presence_absence_config[\"pident_threshold\"]\n",
    "            gene_presence_absence = {} # Create presence-absence dictionary for each gene\n",
    "            genes_found = []\n",
    "            gene_names = get_names_from_fasta(query_sequence_file)\n",
    "            \n",
    "            # Run BLAST if output file does not exist\n",
    "            if not os.path.exists(blast_output_file):\n",
    "                cmd = f\"blastn -query {query_sequence_file} -subject {self.assembly_file} -out {blast_output_file} -outfmt \\\"6 {blast_presence_absence_config['blast_header']}\\\" {blast_presence_absence_config['additional_blast_parameters']}\"\n",
    "                stdout,stderr = self.execute_cmd_and_log(cmd)\n",
    "            else:\n",
    "                self.logger.info(f\"Blast output found at {blast_output_file}, skipping blast.\")\n",
    "            \n",
    "            # Parse BLAST output\n",
    "            self.logger.info(f\"Parsing blast output in {blast_output_file}\")\n",
    "            try:\n",
    "                blast_df = pd.read_csv(blast_output_file, \n",
    "                                       sep='\\t', \n",
    "                                       header = None)\n",
    "            except Exception as e:\n",
    "                self.logger.info(f\"Failed to parse blast output in {blast_output_file}, error message: {e}\")\n",
    "                blast_df = None \n",
    "            if blast_df is not None:\n",
    "                blast_df.columns = blast_presence_absence_config[\"blast_header\"].split(' ')\n",
    "                blast_df[\"plen\"] = blast_df[\"length\"]/blast_df[\"qlen\"]*100\n",
    "                blast_df_filtered = blast_df.query(\"plen > @cov_threshold and pident > @pident_threshold\")\n",
    "                blast_df_unique = blast_df_filtered.sort_values(by=['bitscore'], ascending= False).groupby(\"qseqid\").first()\n",
    "                genes_found =  \",\".join(sorted(list(blast_df_unique.index)))\n",
    "                genes_present = {f\"{analysis_alias}_genes_found\": genes_found}       \n",
    "            else:\n",
    "                blast_df_unique = None\n",
    "            \n",
    "            ##### Not ideal code ####\n",
    "            \n",
    "            # Create presence-absence dictionary for each gene\n",
    "            for gene_name in gene_names:\n",
    "                if gene_name in genes_found:\n",
    "                    gene_presence_absence[gene_name] = \"1\"\n",
    "                else:\n",
    "                    gene_presence_absence[gene_name] = \"0\"\n",
    "            \n",
    "            # Update analysis results\n",
    "            if blast_presence_absence_config[\"results_format\"] == \"string\":\n",
    "                self.analysis_results[analysis_alias] = genes_present\n",
    "            else:\n",
    "                self.analysis_results[analysis_alias] = gene_presence_absence\n",
    "            self.analysis_output_files[analysis_alias] = {\"blast\": blast_output_file}\n",
    "            self.analysis_cleanup(analysis_alias,blast_presence_absence_config[\"files_to_clean\"])\n",
    "            \n",
    "            return({\"genes_present\": genes_found,\n",
    "                    \"gene_presence_absence\": gene_presence_absence,\n",
    "                    #\"blast_df\": blast_df_unique\n",
    "                    })\n",
    "\n",
    "    def __iter__(self):\n",
    "        for analysis_alias in self.analysis_results:\n",
    "            yield(analysis_alias)\n",
    "\n",
    "    def __getitem__(self, analysis_alias):\n",
    "        return({\"results\": self.analysis_results[analysis_alias],\n",
    "                \"output_files\": self.analysis_output_files[analysis_alias]})\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "class analysis_manager(sample_manager.input_manager):\n",
    "    \"\"\"\n",
    "    Analysis manager class that inherits from `input_manager`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_config,\n",
    "                 analysis_settings_config):\n",
    "        \"\"\"\n",
    "        Initialize the analysis utility class by setting up attributes and configurations.\n",
    "\n",
    "        Parameters:\n",
    "        -------------\n",
    "        input_config : yaml file\n",
    "            Configuration file with options to handle samples\n",
    "        analysis_settings_config : dict\n",
    "            Dictionary with analysis to run\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the analysis settings configuration\n",
    "        self.analysis_settings_config: dict = analysis_settings_config\n",
    "        \n",
    "        # Set the base output folder; if not defined in input config, set it to the current directory\n",
    "        if not \"output_folder\" in input_config:\n",
    "            input_config[\"output_folder\"] = \"./\"\n",
    "        \n",
    "        # Get absolute path for the base output folder\n",
    "        self.base_output_folder = os.path.abspath(input_config[\"output_folder\"])\n",
    "        \n",
    "        if not os.path.exists(self.base_output_folder):\n",
    "            os.makedirs(self.base_output_folder)  # Create the output folder if it doesn't exist\n",
    "        \n",
    "        # Initialize the base class (input_manager) with input configuration\n",
    "        super().__init__(input_config)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def init_sample(self,\n",
    "                    attributes):\n",
    "        \"\"\"\n",
    "        Initialize analysis_utility and add class instance to list in analysis_manager.samples\n",
    "\n",
    "        Parameters\n",
    "        -------------\n",
    "        attributes : dict \n",
    "            dictionary containing sample information \n",
    "        \"\"\"\n",
    "        # Determine the output folder for the sample\n",
    "\n",
    "        if \"output_folder\" in attributes:\n",
    "            output_folder = os.path.abspath(attributes[\"output_folder\"])\n",
    "        elif \"sample_name\" in attributes:\n",
    "            output_folder = os.path.join(self.base_output_folder,attributes[\"sample_name\"])\n",
    "        else:\n",
    "            output_folder = self.base_output_folder\n",
    "        # Create an instance of `analysis_utility` with the appropriate attributes\n",
    "            \n",
    "        sample = analysis_utility(attributes,\n",
    "                                  self.base_input_folder,\n",
    "                                  output_folder,\n",
    "                                  self.analysis_settings_config)\n",
    "        # Return the initialized sample\n",
    "        return(sample)\n",
    "\n",
    "    \n",
    "    ########## COPY OF ANOTHER METHOD TO RECHECK #############\n",
    "    def write_to_tsv(self,\n",
    "                     include_metadata = True):\n",
    "        \"\"\"\n",
    "        Write results from all analyses to .tsv file. Includes metadata by default.\n",
    "\n",
    "        Parameters:\n",
    "        -------------\n",
    "\n",
    "        include_metadata : bool\n",
    "            Wether to include metadata\n",
    "        \"\"\"       \n",
    "        \n",
    "        if self.samples:\n",
    "            output_file = os.path.join(self.base_output_folder,\n",
    "                                       \"results_summary.tsv\")\n",
    "            print_dicts = []\n",
    "            for sample in self.samples:\n",
    "                if include_metadata:\n",
    "                    print_dict = sample.metadata.copy()\n",
    "                    if 'Illumina_read_files' in print_dict and isinstance(print_dict['Illumina_read_files'], list):\n",
    "                        print_dict[\"Illumina_read_files\"] = \",\".join(print_dict[\"Illumina_read_files\"])\n",
    "                    print_dict = {'sample_name': print_dict.pop('sample_name'), **print_dict} # Move sample_name to first column before printing for readability\n",
    "                else:\n",
    "                    print_dict = {\"sample_name\": sample.sample_name}\n",
    "                for analysis_alias in sample.analysis_results:\n",
    "                    print_dict.update(sample.analysis_results[analysis_alias])\n",
    "                print_dicts.append(print_dict.copy())\n",
    "            try:\n",
    "                with open(output_file, 'w') as o:\n",
    "                    dict_writer = csv.DictWriter(o, print_dicts[0].keys(), delimiter='\\t')\n",
    "                    dict_writer.writeheader()\n",
    "                    dict_writer.writerows(print_dicts)  \n",
    "                o.close()     \n",
    "                print(f\"Analysis summary written to {output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR while writing analysis summary to {output_file}: {e}\")\n",
    "\n",
    "\n",
    "########## COPY OF ANOTHER METHOD TO RECHECK #############\n",
    "    def setup_log_config(self,\n",
    "                         log_file,\n",
    "                         log_level = \"INFO\"):\n",
    "        logger = logging.getLogger()\n",
    "        logging.basicConfig(\n",
    "            level=log_level,\n",
    "            #filename=str(log_file),\n",
    "            encoding=\"utf-8\",\n",
    "            filemode=\"w\",\n",
    "            format=\"{asctime} - {levelname} - {message}\",\n",
    "            style=\"{\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        )\n",
    "        return(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def get_names_from_fasta(fasta_file):\n",
    "    \"\"\"\n",
    "    Parses a FASTA file and returns list with headers \n",
    "    matching what would appear as qseqid or sseqid in blast\n",
    "\n",
    "    Parameters:\n",
    "    -------------\n",
    "        fasta_file : str\n",
    "            Path to the FASTA file.\n",
    "\n",
    "    Returns:\n",
    "    -------------\n",
    "       sequence_names: dict\n",
    "            Dictionary with sequence headers as keys.\n",
    "    \"\"\"\n",
    "    with open(fasta_file) as f:\n",
    "        sequence_names = {}\n",
    "        for line in f:\n",
    "            if line[0] == \">\":\n",
    "                line = line.rstrip(\"\\n\")[1:]\n",
    "                sequence_names[line.split()[0]] = line\n",
    "    return(sequence_names)\n",
    "\n",
    "def print_analysis_options(analysis_config):\n",
    "    \"\"\"\n",
    "    Function to print available analyses based on the configuration\n",
    "   \n",
    "    Parameters:\n",
    "    -------------\n",
    "    analysis_config: dict\n",
    "            Configuration settings for the analysis, including analysis type and parameters.\n",
    "    \"\"\"\n",
    "    print(\"The following analyses are available:\\n\")\n",
    "    analyses_to_run = analysis_config[\"analyses_to_run\"]\n",
    "    \n",
    "    # If analyses are defined as a dictionary, print each with its description and output files\n",
    "    if isinstance(analyses_to_run,dict):\n",
    "        for analysis,description in analyses_to_run.items():\n",
    "            print(f\"{analysis}: {description}\")\n",
    "            try:\n",
    "                # Print output files to be cleaned after the analysis\n",
    "                output_files_print = \"\\n    - \".join(analysis_config[analysis][\"files_to_clean\"])\n",
    "                print(f\" output_files:\\n    - {output_files_print}\\n\")\n",
    "            except:\n",
    "                print(\"\\n\")\n",
    "    \n",
    "    # If analyses are a list, print each analysis name\n",
    "    elif isinstance(analyses_to_run,list):\n",
    "        for analysis in analyses_to_run:\n",
    "            print(f\"{analysis}: {description}\")\n",
    "            try:\n",
    "                output_files_print = \"\\n    - \".join(analysis_config[analysis][\"files_to_clean\"])\n",
    "                print(f\" output_files:\\n    - {output_files_print}\\n\")\n",
    "            except:\n",
    "                print(\"\\n\")\n",
    "    else:\n",
    "        print(\"No analyses found in provided config\")\n",
    "\n",
    "\n",
    "def update_cleanup_config(analysis_config,\n",
    "                          files_to_keep_string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to update the cleanup configuration to keep specified files\n",
    "    \n",
    "    Parameters:\n",
    "    -------------\n",
    "    analysis_config: dict\n",
    "            Configuration settings for the analysis, including analysis type and parameters.\n",
    "    files_to_keep_string: str\n",
    "            Files to keep\n",
    "    \"\"\"\n",
    "    files_to_keep_list = files_to_keep_string.split(' ')\n",
    "    for element in files_to_keep_list:\n",
    "        element_split = element.split(':')\n",
    "        if element_split[0] in analysis_config:\n",
    "            if len(element_split) == 1:\n",
    "                analysis_config[element][\"files_to_clean\"] = []\n",
    "            else:\n",
    "                analysis_config[element_split[0]][\"files_to_clean\"] = list(set(analysis_config[element_split[0]][\"files_to_clean\"])-set(element_split[1].split(',')))\n",
    "    return(analysis_config)\n",
    "\n",
    "def update_cli_input_config(input_config,\n",
    "                            analysis_config,\n",
    "                            samplesheet,\n",
    "                            input_folder,\n",
    "                            assembly_file,\n",
    "                            Illumina_read_files,\n",
    "                            Nanopore_read_file,\n",
    "                            output_folder,\n",
    "                            load_from_samplesheet,\n",
    "                            analyses_to_run,\n",
    "                            keep_files):\n",
    "    \n",
    "    \"\"\"\n",
    "    Updates input and analysis configuration dictionaries based on provided command-line arguments.\n",
    "    \n",
    "    Parameters:\n",
    "    -------------\n",
    "        input_config (dict): \n",
    "                The main configuration dictionary for input parameters.\n",
    "        analysis_config (dict): \n",
    "                The configuration dictionary for analysis settings.\n",
    "        samplesheet (str or None): \n",
    "                Path to the samplesheet file.\n",
    "        input_folder (str or None): \n",
    "                Path to the folder containing input data.\n",
    "        assembly_file (str or None): \n",
    "                Path to the assembly file.\n",
    "        Illumina_read_files (list or None): \n",
    "                List of Illumina read file paths.\n",
    "        Nanopore_read_file (str or None): \n",
    "                Path to the Nanopore read file.\n",
    "        output_folder (str or None): \n",
    "                Path to the output folder.\n",
    "        load_from_samplesheet (bool): \n",
    "                Whether to load configuration directly from the samplesheet.\n",
    "        analyses_to_run (str): \n",
    "                Comma-separated list of analyses to run, or \"all\" to run all analyses.\n",
    "        keep_files (str or None): \n",
    "                Specifies files to retain after cleanup.\n",
    "        \n",
    "    Returns:\n",
    "    -------------\n",
    "        tuple: Updated input_config and analysis_config dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    # Update the samplesheet if it was provided\n",
    "    if samplesheet is not None:\n",
    "        input_config[\"samplesheet\"] = samplesheet\n",
    "        # If no input folder is specified, load data from the samplesheet\n",
    "        if input_folder is None:\n",
    "            load_from_samplesheet = True\n",
    "    \n",
    "    # Update additional input files and folders in input_config if they are provided\n",
    "    # Set the assembly file if it was specified\n",
    "    if assembly_file is not None:\n",
    "        input_config[\"assembly_file\"] = assembly_file\n",
    "    \n",
    "    # Set the Illumina read files if they were specified\n",
    "    if Illumina_read_files is not None:\n",
    "        input_config[\"Illumina_read_files\"] = Illumina_read_files\n",
    "    \n",
    "    # Set the Nanopore read file if it was specified\n",
    "    if Nanopore_read_file is not None:\n",
    "        input_config[\"Nanopore_read_file\"] = Nanopore_read_file\n",
    "    \n",
    "    # Set the input folder if it was specified\n",
    "    if input_folder is not None:\n",
    "        input_config[\"input_folder\"] = input_folder\n",
    "        # Enable loading from the specified folder if not loading from sampleshee\n",
    "        if not input_config[\"load_from_samplesheet\"]:\n",
    "            input_config[\"load_from_folder\"] = True\n",
    "    \n",
    "    # Set the output folder if it was specified\n",
    "    if output_folder is not None:\n",
    "        input_config[\"output_folder\"] = output_folder\n",
    "    \n",
    "    # Update analysis_config to specify which analyses to run\n",
    "    # If `analyses_to_run` is not set to \"all\", update it with the intersection of provided analyses and existing analyses\n",
    "    if not analyses_to_run == \"all\":\n",
    "        analysis_config[\"analyses_to_run\"] = list(set(analysis_config[\"analyses_to_run\"]).intersection(analyses_to_run.split(',')))\n",
    "    \n",
    "    # Update files to keep based on `keep_files` argument\n",
    "    if keep_files is not None:\n",
    "        analysis_config = update_cleanup_config(analysis_config,keep_files)\n",
    "    input_config[\"load_from_samplesheet\"] = load_from_samplesheet\n",
    "    \n",
    "    # Return the updated configuration dictionaries\n",
    "    return(input_config,\n",
    "           analysis_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "The log parser unit test parses an rclone log found in tests/rclone_log_parser_test_log.txt. It verifies basic information like number of succesful and unsuccesful transfers in the log, and prints a tsv-formatted summary table of each transfer instance.\n",
    "\n",
    "Should there be some try/except statements here, so the user get some kind of direct feedback like \"all tests passed\" (or not, if thats the case)? Right now, I dont think its very obvious whether the test was successful or not, from the user-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def unit_test_single():\n",
    "    config = core.get_config()\n",
    "    example_sample = analysis_utility({\"sample_name\":\"GAS-2022-1029\",\n",
    "                                       \"assembly_file\":\"examples/GAS-2022-1029.fasta\",\n",
    "                                       \"Illumina_read_files\":[\"examples/GAS-2022-1029_S42_L555_R1_001.fastq.gz\",\"examples/GAS-2022-1029_S42_L555_R2_001.fastq.gz\"]},\n",
    "                                       input_folder = False,\n",
    "                                       output_folder = \"output/\",\n",
    "                                       analysis_config = config[\"analysis_settings\"][\"Spyogenes\"],\n",
    "                                       )\n",
    "    assert(example_sample.sample_name == \"GAS-2022-1029\")\n",
    "    assert(len(example_sample.Illumina_read_files) == 2)\n",
    "    assert(not example_sample.Nanopore_read_file)\n",
    "\n",
    "def unit_test_single_2():\n",
    "    config = core.get_config()\n",
    "    example_sample = analysis_utility({\"sample_name\":\"GAS-2024-0773\",\n",
    "                                       \"assembly_file\":\"GAS-2024-0773.fasta\",\n",
    "                                       \"Illumina_read_files\":[\"GAS-2024-0773_S35_L555_R1_001.fastq.gz\",\"GAS-2024-0773_S35_L555_R2_001.fastq.gz\"]},\n",
    "                                       input_folder = \"examples\",\n",
    "                                       output_folder = \"output/\",\n",
    "                                       analysis_config = config[\"analysis_settings\"][\"Spyogenes\"],\n",
    "                                       )\n",
    "    assert(example_sample.sample_name == \"GAS-2024-0773\")\n",
    "    assert(len(example_sample.Illumina_read_files) == 2)\n",
    "    assert(not example_sample.Nanopore_read_file)\n",
    "\n",
    "def unit_test_single_3():\n",
    "    config = core.get_config()\n",
    "    example_sample = analysis_utility({\"sample_name\":\"GAS-2022-1029\",\n",
    "                                       \"assembly_file\":\"examples/GAS-2022-1029.fasta\",\n",
    "                                       \"Illumina_read_files\":[\"examples/GAS-2022-1029_S42_L555_R1_001.fastq.gz\",\"examples/GAS-2022-1029_S42_L555_R2_001.fastq.gz\"],\n",
    "                                       \"samplesheet\": \"examples/samplesheet.tsv\"},\n",
    "                                       input_folder = False,\n",
    "                                       output_folder = \"output/\",\n",
    "                                       analysis_config = config[\"analysis_settings\"][\"Spyogenes\"]\n",
    "                                       )\n",
    "    assert(example_sample.sample_name == \"GAS-2022-1029\")\n",
    "    assert(len(example_sample.Illumina_read_files) == 2)\n",
    "    assert(not example_sample.Nanopore_read_file)\n",
    "    print(example_sample.metadata)\n",
    "\n",
    "\n",
    "def unit_test_from_folder():\n",
    "    config = core.get_config()\n",
    "    input_config =  config[\"input_manager\"]\n",
    "    input_config[\"load_from_folder\"] = True\n",
    "    input_config[\"input_folder\"] = \"examples/\"\n",
    "    input_config[\"output_folder\"] = \"output_from_folder/\"\n",
    "    input_config[\"analysis_config\"] = config[\"analysis_settings\"][\"Spyogenes\"]\n",
    "    test = analysis_manager(input_config,config[\"analysis_settings\"][\"Spyogenes\"])\n",
    "\n",
    "def unit_test_from_samplesheet():\n",
    "    config = core.get_config()\n",
    "    input_config =  config[\"input_manager\"]\n",
    "    input_config[\"load_from_samplesheet\"] = True\n",
    "    input_config[\"samplesheet\"] = \"examples/samplesheet.tsv\"\n",
    "    input_config[\"output_folder\"] = \"output_from_samplesheet/\"\n",
    "    input_config[\"analysis_config\"] = config[\"analysis_settings\"][\"Spyogenes\"]\n",
    "    test = analysis_manager(input_config,config[\"analysis_settings\"][\"Spyogenes\"])\n",
    "    print(test.__dict__)\n",
    "    for x in test:\n",
    "        print(x.__dict__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit_test_single()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit_test_single_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sample_name': 'GAS-2022-1029', 'assembly_file': 'examples/GAS-2022-1029.fasta', 'Illumina_read_files': ['examples/GAS-2022-1029_S42_L555_R1_001.fastq.gz', 'examples/GAS-2022-1029_S42_L555_R2_001.fastq.gz'], 'samplesheet': 'examples/samplesheet.tsv'}\n"
     ]
    }
   ],
   "source": [
    "# unit_test_single_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unit_test_from_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis_settings_config': {'analyses_to_run': {'emm_typing': 'Typing of M protein gene using blast against CDC curated alleles', 'assembly_lineage_determination': 'Lineage determination based on presence of specified SNPs when mapping genome assembly against reference genome', 'resistance_gene_detection': 'Presence of resistance gens tetM, ermA and ermB', 'virulence_gene_detection': 'Presence of Streptococcal virulence genes from the Virulence Factor Database'}, 'assembly_lineage_determination': {'alias': 'assembly_lineage_determination', 'percent_snp_threshold': 50, 'files_to_clean': ['delta', 'filtered_delta', 'frankenfasta'], 'reference_fasta_file': './resources/lineage_determination/MGAS5005.fasta', 'lineage_variant_file': './resources/lineage_determination/Spyogenes_LOCs.tsv'}, 'emm_typing': {'alias': 'emm_typing', 'emm_allele_file': './resources/emm_typing/emm_alleles.fasta', 'emm_cluster_file': './resources/emm_typing/emm_enn_mrp_subgroups.txt', 'blast_header': 'qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore', 'files_to_clean': ['blast']}, 'resistance_gene_detection': {'pident_threshold': 80, 'cov_threshold': 80, 'additional_blast_parameters': '', 'blast_header': 'qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore', 'results_format': 'matrix', 'files_to_clean': ['blast'], 'alias': 'resistance_genes', 'query_fasta_file': './resources/AMR/AMR_genes.fasta'}, 'virulence_gene_detection': {'pident_threshold': 80, 'cov_threshold': 80, 'additional_blast_parameters': '', 'blast_header': 'qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore', 'results_format': 'matrix', 'files_to_clean': ['blast'], 'alias': 'virulence_genes', 'query_fasta_file': './resources/virulence_factors/VFDB_genes.fasta'}, 'Illumina_lineage_determination': {'alias': 'Illumina_lineage_determination', 'reference_fasta_file': './resources/lineage_determination/MGAS5005.fasta', 'lineage_variant_file': './resources/lineage_determination/Spyogenes_LOCs.tsv', 'files_to_clean': ['vcf', 'bam', 'bam_index']}}, 'base_output_folder': '/Users/b246838/Documents/git.repositories/streptofile/output_from_samplesheet', 'base_input_folder': '/Users/b246838/Documents/git.repositories/streptofile/examples', 'metadata_file': '/Users/b246838/Documents/git.repositories/streptofile/examples/samplesheet.tsv', 'metadata': {'GAS-2022-1029': {'Illumina_read_files': 'GAS-2022-1029_S42_L555_R1_001.fastq.gz,GAS-2022-1029_S42_L555_R2_001.fastq.gz', 'Nanopore_read_file': 'GAS-2022-1029_nanopore.fastq.gz', 'assembly_file': 'GAS-2022-1029.fasta', 'organism': 'Streptococcus pyogenes', 'variant': 'M1DK', 'notes': 'speC positive'}, 'GAS-2023-0253': {'Illumina_read_files': 'GAS-2023-0253_S74_L555_R1_001.fastq.gz,GAS-2023-0253_S74_L555_R2_001.fastq.gz', 'Nanopore_read_file': 'GAS-2023-0253_nanopore.fastq.gz', 'assembly_file': 'GAS-2023-0253.fasta', 'organism': 'Streptococcus pyogenes', 'variant': 'M1UK', 'notes': nan}}, 'samples': [<__main__.analysis_utility object>, <__main__.analysis_utility object>]}\n",
      "{'input_folder': '/Users/b246838/Documents/git.repositories/streptofile/examples', 'sample_name': 'GAS-2022-1029', 'assembly_file': '/Users/b246838/Documents/git.repositories/streptofile/examples/GAS-2022-1029.fasta', 'Illumina_read_files': ['/Users/b246838/Documents/git.repositories/streptofile/examples/GAS-2022-1029_S42_L555_R1_001.fastq.gz', '/Users/b246838/Documents/git.repositories/streptofile/examples/GAS-2022-1029_S42_L555_R2_001.fastq.gz'], 'Nanopore_read_file': '/Users/b246838/Documents/git.repositories/streptofile/examples/GAS-2022-1029_nanopore.fastq.gz', 'metadata': {'Illumina_read_files': 'GAS-2022-1029_S42_L555_R1_001.fastq.gz,GAS-2022-1029_S42_L555_R2_001.fastq.gz', 'Nanopore_read_file': 'GAS-2022-1029_nanopore.fastq.gz', 'assembly_file': 'GAS-2022-1029.fasta', 'organism': 'Streptococcus pyogenes', 'variant': 'M1DK', 'notes': 'speC positive', 'sample_name': 'GAS-2022-1029'}, 'analysis_results': {}, 'analysis_output_files': {}, 'output_folder': '/Users/b246838/Documents/git.repositories/streptofile/output_from_samplesheet/GAS-2022-1029', 'analysis_config': {'analyses_to_run': {'emm_typing': 'Typing of M protein gene using blast against CDC curated alleles', 'assembly_lineage_determination': 'Lineage determination based on presence of specified SNPs when mapping genome assembly against reference genome', 'resistance_gene_detection': 'Presence of resistance gens tetM, ermA and ermB', 'virulence_gene_detection': 'Presence of Streptococcal virulence genes from the Virulence Factor Database'}, 'assembly_lineage_determination': {'alias': 'assembly_lineage_determination', 'percent_snp_threshold': 50, 'files_to_clean': ['delta', 'filtered_delta', 'frankenfasta'], 'reference_fasta_file': './resources/lineage_determination/MGAS5005.fasta', 'lineage_variant_file': './resources/lineage_determination/Spyogenes_LOCs.tsv'}, 'emm_typing': {'alias': 'emm_typing', 'emm_allele_file': './resources/emm_typing/emm_alleles.fasta', 'emm_cluster_file': './resources/emm_typing/emm_enn_mrp_subgroups.txt', 'blast_header': 'qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore', 'files_to_clean': ['blast']}, 'resistance_gene_detection': {'pident_threshold': 80, 'cov_threshold': 80, 'additional_blast_parameters': '', 'blast_header': 'qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore', 'results_format': 'matrix', 'files_to_clean': ['blast'], 'alias': 'resistance_genes', 'query_fasta_file': './resources/AMR/AMR_genes.fasta'}, 'virulence_gene_detection': {'pident_threshold': 80, 'cov_threshold': 80, 'additional_blast_parameters': '', 'blast_header': 'qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore', 'results_format': 'matrix', 'files_to_clean': ['blast'], 'alias': 'virulence_genes', 'query_fasta_file': './resources/virulence_factors/VFDB_genes.fasta'}, 'Illumina_lineage_determination': {'alias': 'Illumina_lineage_determination', 'reference_fasta_file': './resources/lineage_determination/MGAS5005.fasta', 'lineage_variant_file': './resources/lineage_determination/Spyogenes_LOCs.tsv', 'files_to_clean': ['vcf', 'bam', 'bam_index']}}}\n",
      "{'input_folder': '/Users/b246838/Documents/git.repositories/streptofile/examples', 'sample_name': 'GAS-2023-0253', 'assembly_file': '/Users/b246838/Documents/git.repositories/streptofile/examples/GAS-2023-0253.fasta', 'Illumina_read_files': ['/Users/b246838/Documents/git.repositories/streptofile/examples/GAS-2023-0253_S74_L555_R1_001.fastq.gz', '/Users/b246838/Documents/git.repositories/streptofile/examples/GAS-2023-0253_S74_L555_R2_001.fastq.gz'], 'Nanopore_read_file': '/Users/b246838/Documents/git.repositories/streptofile/examples/GAS-2023-0253_nanopore.fastq.gz', 'metadata': {'Illumina_read_files': 'GAS-2023-0253_S74_L555_R1_001.fastq.gz,GAS-2023-0253_S74_L555_R2_001.fastq.gz', 'Nanopore_read_file': 'GAS-2023-0253_nanopore.fastq.gz', 'assembly_file': 'GAS-2023-0253.fasta', 'organism': 'Streptococcus pyogenes', 'variant': 'M1UK', 'notes': nan, 'sample_name': 'GAS-2023-0253'}, 'analysis_results': {}, 'analysis_output_files': {}, 'output_folder': '/Users/b246838/Documents/git.repositories/streptofile/output_from_samplesheet/GAS-2023-0253', 'analysis_config': {'analyses_to_run': {'emm_typing': 'Typing of M protein gene using blast against CDC curated alleles', 'assembly_lineage_determination': 'Lineage determination based on presence of specified SNPs when mapping genome assembly against reference genome', 'resistance_gene_detection': 'Presence of resistance gens tetM, ermA and ermB', 'virulence_gene_detection': 'Presence of Streptococcal virulence genes from the Virulence Factor Database'}, 'assembly_lineage_determination': {'alias': 'assembly_lineage_determination', 'percent_snp_threshold': 50, 'files_to_clean': ['delta', 'filtered_delta', 'frankenfasta'], 'reference_fasta_file': './resources/lineage_determination/MGAS5005.fasta', 'lineage_variant_file': './resources/lineage_determination/Spyogenes_LOCs.tsv'}, 'emm_typing': {'alias': 'emm_typing', 'emm_allele_file': './resources/emm_typing/emm_alleles.fasta', 'emm_cluster_file': './resources/emm_typing/emm_enn_mrp_subgroups.txt', 'blast_header': 'qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore', 'files_to_clean': ['blast']}, 'resistance_gene_detection': {'pident_threshold': 80, 'cov_threshold': 80, 'additional_blast_parameters': '', 'blast_header': 'qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore', 'results_format': 'matrix', 'files_to_clean': ['blast'], 'alias': 'resistance_genes', 'query_fasta_file': './resources/AMR/AMR_genes.fasta'}, 'virulence_gene_detection': {'pident_threshold': 80, 'cov_threshold': 80, 'additional_blast_parameters': '', 'blast_header': 'qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore', 'results_format': 'matrix', 'files_to_clean': ['blast'], 'alias': 'virulence_genes', 'query_fasta_file': './resources/virulence_factors/VFDB_genes.fasta'}, 'Illumina_lineage_determination': {'alias': 'Illumina_lineage_determination', 'reference_fasta_file': './resources/lineage_determination/MGAS5005.fasta', 'lineage_variant_file': './resources/lineage_determination/Spyogenes_LOCs.tsv', 'files_to_clean': ['vcf', 'bam', 'bam_index']}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# unit_test_from_samplesheet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following analyses are available:\n",
      "\n",
      "emm_typing: Typing of M protein gene using blast against CDC curated alleles\n",
      " output_files:\n",
      "    - blast\n",
      "\n",
      "assembly_lineage_determination: Lineage determination based on presence of specified SNPs when mapping genome assembly against reference genome\n",
      " output_files:\n",
      "    - delta\n",
      "    - filtered_delta\n",
      "    - frankenfasta\n",
      "\n",
      "resistance_gene_detection: Presence of resistance gens tetM, ermA and ermB\n",
      " output_files:\n",
      "    - blast\n",
      "\n",
      "virulence_gene_detection: Presence of Streptococcal virulence genes from the Virulence Factor Database\n",
      " output_files:\n",
      "    - blast\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# config = core.get_config()\n",
    "\n",
    "# print_analysis_options(config[\"analysis_settings\"][\"Spyogenes\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cli function\n",
    "\n",
    "This function is responsible for the execution of the current notebook, including parsing arguments.\n",
    "\n",
    "The current version of the code require specification of an input log-file. It can be specified using the --input argument, or by providing a config-file as argument (with the log-file specified). If neither argument is specified, the script will exit with an error.\n",
    "\n",
    "It is also possible to run a \"unit-test\", by specyfying the \"unit_test\" flag. In that case, the script will run on the test log-file provided with the repo, and exit without doing anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# This is included at the end to ensure when you run through your notebook the code is also transferred to the module and isn't just a notebook\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
